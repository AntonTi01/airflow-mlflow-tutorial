{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import yaml\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "from googleapiclient.discovery import build\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import re\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import f1_score, silhouette_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация клиента YouTube API\n",
    "def initialize_youtube(YOUTUBE_API_KEY):\n",
    "    return build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)\n",
    "\n",
    "# Функция для получения ID видео по ключевым словам\n",
    "def get_video_ids(youtube, query, count_video=10):\n",
    "    \"\"\"\n",
    "    Поиск видео по ключевым словам\n",
    "    \"\"\"\n",
    "    search_response = youtube.search().list(\n",
    "        q=query,\n",
    "        part='id',\n",
    "        maxResults=count_video,\n",
    "        type='video'\n",
    "    ).execute()\n",
    "    \n",
    "    video_ids = [item['id']['videoId'] for item in search_response.get('items', [])]\n",
    "    return video_ids\n",
    "\n",
    "# Получение данных о комментариях\n",
    "def get_data(YOUTUBE_API_KEY, videoId, maxResults, nextPageToken):\n",
    "    \"\"\"\n",
    "    Получение информации со страницы с видео\n",
    "    \"\"\"\n",
    "    YOUTUBE_URI = 'https://www.googleapis.com/youtube/v3/commentThreads?key={KEY}&textFormat=plainText&' + \\\n",
    "        'part=snippet&videoId={videoId}&maxResults={maxResults}&pageToken={nextPageToken}'\n",
    "    format_youtube_uri = YOUTUBE_URI.format(KEY=YOUTUBE_API_KEY,\n",
    "                                            videoId=videoId,\n",
    "                                            maxResults=maxResults,\n",
    "                                            nextPageToken=nextPageToken)\n",
    "    content = requests.get(format_youtube_uri).text\n",
    "    data = json.loads(content)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_text_of_comment(data):\n",
    "    \"\"\"\n",
    "    Получение комментариев из полученных данных под одним видео\n",
    "    \"\"\"\n",
    "    comms = set()\n",
    "    for item in data['items']:\n",
    "        comm = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "        comms.add(comm)\n",
    "    return comms\n",
    "\n",
    "\n",
    "# Основная функция для получения всех комментариев\n",
    "def get_all_comments(YOUTUBE_API_KEY, query, count_video=10, limit=30, maxResults=10, nextPageToken=''):\n",
    "    \"\"\"\n",
    "    Выгрузка maxResults комментариев\n",
    "    \"\"\"\n",
    "    youtube = initialize_youtube(YOUTUBE_API_KEY)\n",
    "    videoIds = get_video_ids(youtube, query, count_video)\n",
    "\n",
    "    comments_all = []\n",
    "    for id_video in videoIds:\n",
    "        try:\n",
    "            data = get_data(YOUTUBE_API_KEY, id_video, maxResults=maxResults, nextPageToken=nextPageToken)\n",
    "            comment = list(get_text_of_comment(data))\n",
    "            comments_all.append(comment)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching comments for video ID {id_video}: {e}\")\n",
    "            continue\n",
    "    comments = sum(comments_all, [])\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join('/Users/forcemajor01/data_science/work_place/other/airflow-mlflow-tutorial/configs/params_all.yaml')\n",
    "config = yaml.safe_load(open(config_path))['train']\n",
    "SEED = config['SEED']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SEED': 10,\n",
       " 'clustering': {'affinity': 'cosine',\n",
       "  'count_max_clusters': 15,\n",
       "  'silhouette_metric': 'euclidean'},\n",
       " 'comments': {'YOUTUBE_API_KEY': 'AIzaSyBLU5mFczWyGRHq4HLpm9OzENB05l7RP3w',\n",
       "  'count_video': 50,\n",
       "  'limit': 30,\n",
       "  'maxResults': 250,\n",
       "  'nextPageToken': '',\n",
       "  'query': 'дата сайенс'},\n",
       " 'cross_val': {'test_size': 0.3},\n",
       " 'dir_folder': '/Users/forcemajor01/data_science/work_place/other/airflow-mlflow-tutorial',\n",
       " 'model': {'class_weight': 'balanced'},\n",
       " 'model_lr': 'LogisticRegression',\n",
       " 'model_vec': 'vector_tfidf',\n",
       " 'name_experiment': 'my_first',\n",
       " 'stopwords': 'russian',\n",
       " 'tf_model': {'max_features': 300}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = get_all_comments(**config['comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Так на самом деле',\n",
       " 'Никакого дефицита в DS на самом деле нет.\\n\\nЧто касается entry-level позиций, то они закрываются либо по знакомствам, либо с дичайшим конкурсом в пользу ребят с топовых кафедр по матеше/проге.\\n\\n Проблема у работодателей разве что может быть, как и в США, в поиске спецов на research позиции, хотя деньги там большие',\n",
       " 'Работаю в сфере не связанной с IT и совершенно далекой от математики, зарплата мои потребности покрывает, но захотелось чего-то другого - по итогу пошел на курсы по DS в одну известную контору. \\nЗаканчиваю 2-й год по CV. Пришел к выводу, что нужна вышка, иначе будут проблемы с поиском новой работы. \\nСдал экзамены и поступил в магистратуру одного очень известного ВУЗа (по результатам экзаменов в верхней 1/3 абитуриентов). С сентября начинается учеба. \\nНаправление очень нравится, с математикой, правда, сложно, но буду пытаться. \\nСо своего опыта обучения скажу так - дата-сайентисты свои зарплаты получают не просто так, это реально очень сложное направление. Без математики, без английского, без чтения научных статей тут никуда. \\nДа… если кому интересно, мне скоро 45 :) Не знаю, как у меня все сложится, но при этом я не жалею ни одного рубля, потраченного на учебу. \\nВсем удачи!',\n",
       " '0:16 вот все вакансии и кончились)))',\n",
       " 'КвартАл! КвартАл!\\nА не квАртал.\\nОстальное - бaza)',\n",
       " 'Автоматизация и правда растет. Так что через 4 года можно будет спокойно агентство открыть или на инфобиз перейти...',\n",
       " 'А если я люблю не математику, докапывание до итстинны, програмирование, а просто люблю деньги, подойдёт профессия?',\n",
       " 'Ну понятно что вы говорите что специалисты data science нужны, что есть дефицит, хотя его нет так как специлистов прям дохрена. У вас самой ведь курс по ds',\n",
       " 'А я вот книжку прикупил \"Математика в машинном обучении\" авторы: Дайзенрот Марк Питер, Альдо Фейзал А., Чен Сунь Он для новичка подойдет)',\n",
       " 'о. мемно всё. как круто. наконецто весело смотреть']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    \"\"\"\n",
    "    Удаление эмоджи из текста\n",
    "    \"\"\"\n",
    "    emoji_pattern = re.compile(\"[\"u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u'\\U00010000-\\U0010ffff'\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\u3030\"\n",
    "                               u\"\\ufe0f\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "\n",
    "def remove_links(string):\n",
    "    \"\"\"\n",
    "    Удаление ссылок\n",
    "    \"\"\"\n",
    "    string = re.sub(r'http\\S+', '', string)  # remove http links\n",
    "    string = re.sub(r'bit.ly/\\S+', '', string)  # rempve bitly links\n",
    "    string = re.sub(r'www\\S+', '', string)  # rempve bitly links\n",
    "    string = string.strip('[link]')  # remove [links]\n",
    "    return string\n",
    "\n",
    "\n",
    "def preprocessing(string, stopwords, stem):\n",
    "    \"\"\"\n",
    "    Простой препроцессинг текста, очистка, лематизация, удаление коротких слов\n",
    "    \"\"\"\n",
    "    string = remove_emoji(string)\n",
    "    string = remove_links(string)\n",
    "\n",
    "    # удаление символов \"\\r\\n\"\n",
    "    str_pattern = re.compile(\"\\r\\n\")\n",
    "    string = str_pattern.sub(r'', string)\n",
    "\n",
    "    # очистка текста от символов\n",
    "    string = re.sub('(((?![а-яА-Я ]).)+)', ' ', string)\n",
    "    # лематизация\n",
    "    string = ' '.join([\n",
    "        re.sub('\\\\n', '', ' '.join(stem.lemmatize(s))).strip()\n",
    "        for s in string.split()\n",
    "    ])\n",
    "    # удаляем слова короче 3 символов\n",
    "    string = ' '.join([s for s in string.split() if len(s) > 3])\n",
    "    # удаляем стоп-слова\n",
    "    string = ' '.join([s for s in string.split() if s not in stopwords])\n",
    "    return string\n",
    "\n",
    "\n",
    "def get_clean_text(data, stopwords):\n",
    "    \"\"\"\n",
    "    Получение текста в преобразованной после очистки\n",
    "    матричном виде, а также модель векторизации\n",
    "    \"\"\"\n",
    "    # Простой препроцессинг текста\n",
    "    stem = Mystem()\n",
    "    comments = [preprocessing(x, stopwords, stem) for x in data]\n",
    "    # Удаление комментов, которые имеют меньше, чем 5 слов\n",
    "    comments = [y for y in comments if len(y.split()) > 5]\n",
    "    #common_texts = [i.split(' ') for i in comments]\n",
    "    return comments\n",
    "\n",
    "\n",
    "def vectorize_text(data, tfidf):\n",
    "    \"\"\"\n",
    "    Получение матрицы кол-ва слов в комменариях\n",
    "    Очистка от пустых строк\n",
    "    \"\"\"\n",
    "    # Векторизация\n",
    "    X_matrix = tfidf.transform(data).toarray()\n",
    "    # Удаляем строки в матрице с пустыми значениями\n",
    "    mask = (np.nan_to_num(X_matrix) != 0).any(axis=1)\n",
    "    return X_matrix[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_clean = get_clean_text(comments, stopwords.words(config['stopwords']))\n",
    "tfidf = TfidfVectorizer(**config['tf_model']).fit(comments_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['никакой дефицит самый дело касаться позиция закрываться либо знакомство либо дикий конкурс польза ребята топовый кафедра матеш прога проблема работодатель поиск спец позиция хотя деньги большой',\n",
       " 'работать сфера связывать совершенно далекий математик зарплата потребность покрывать захотеться итог пойти курсы известный контора заканчивать приходить вывод нужный вышка иначе проблема поиск новый работа сдавать экзамен поступать магистратура очень известный результат экзамен верхний абитуриент сентябрь начинаться учеба направление очень нравиться математика правда сложно пытаться свой опыт обучение сказать дата сайентист свой зарплата получать просто реально очень сложный направление математик английский чтение научный статья никуда интересно скоро знать складываться жалеть рубль потратить учеба удача',\n",
       " 'автоматизация правда расти спокойно агентство открывать инфобиз переходить',\n",
       " 'любить математика докапывание итстинный програмирование просто любить деньги подойд профессия',\n",
       " 'понятно говорить специалист нужный дефицит хотя специлист прямой дохрить самый курс',\n",
       " 'книжка прикупать математика машинный обучение автор дайзенрот марк питер альдо фейзал сунуть новичок подходить',\n",
       " 'смотреть месяц мочь находить работа везде нужный релевантный опыт джуна востребовать',\n",
       " 'смешить аргумент дескать хороший специалист важный работа равно находить переводить русский значит нужно работать мало деньги вакнсия оставаться',\n",
       " 'любить математика пока доходить спектральный разложение матрица пока оставлять учиться финансы математика нужный хотеть будущее связывать сфера финансы наука данный обязательно записываться курс хороший ребята направление работать начинаться интересно спасибо видео немного мотивировать',\n",
       " 'ради любопытство зайти вакансия вакансия неделя резюме искать похоже особый дефицит']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_clean[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_matrix = vectorize_text(comments_clean, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1132, 300)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['автор', 'алгоритм', 'анализ', 'аналитик', 'аналитика',\n",
       "       'анастасия', 'английский', 'бабушкин', 'база', 'базовый'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names_out()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(data, count_max_clusters, random_state, affinity,\n",
    "                 silhouette_metric):\n",
    "    \"\"\"\n",
    "    Подбор наилучшего числа кластеров, возвращает полученные кластера тематик\n",
    "    \"\"\"\n",
    "    cluster_labels = {}\n",
    "    silhouette_mean = []\n",
    "\n",
    "    for i in range(2, count_max_clusters, 1):\n",
    "        clf = SpectralClustering(n_clusters=i,\n",
    "                                 affinity=affinity,\n",
    "                                 random_state=random_state)\n",
    "        #clf = KMeans(n_clusters=n, max_iter=1000, n_init=1)\n",
    "        clf.fit(data)\n",
    "        labels = clf.labels_\n",
    "        cluster_labels[i] = labels\n",
    "        silhouette_mean.append(\n",
    "            silhouette_score(data, labels, metric=silhouette_metric))\n",
    "    n_clusters = silhouette_mean.index(max(silhouette_mean)) + 2\n",
    "    return cluster_labels[n_clusters]\n",
    "\n",
    "\n",
    "def get_f1_score(y_test, y_pred, unique_cluster_labels):\n",
    "    \"\"\"\n",
    "    Возращает результат обучения классификатора по тематикам\n",
    "    \"\"\"\n",
    "    return f1_score(\n",
    "        y_test, y_pred,\n",
    "        average='macro') \\\n",
    "        if len(unique_cluster_labels) > 2 \\\n",
    "        else f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = get_clusters(X_matrix,\n",
    "                                 random_state=SEED,\n",
    "                                 **config['clustering'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SEED': 10,\n",
       " 'clustering': {'affinity': 'cosine',\n",
       "  'count_max_clusters': 15,\n",
       "  'silhouette_metric': 'euclidean'},\n",
       " 'comments': {'YOUTUBE_API_KEY': 'AIzaSyBLU5mFczWyGRHq4HLpm9OzENB05l7RP3w',\n",
       "  'count_video': 50,\n",
       "  'limit': 30,\n",
       "  'maxResults': 250,\n",
       "  'nextPageToken': '',\n",
       "  'query': 'дата сайенс'},\n",
       " 'cross_val': {'test_size': 0.3},\n",
       " 'dir_folder': '/Users/forcemajor01/data_science/work_place/other/airflow-mlflow-tutorial',\n",
       " 'model': {'class_weight': 'balanced'},\n",
       " 'model_lr': 'LogisticRegression',\n",
       " 'model_vec': 'vector_tfidf',\n",
       " 'name_experiment': 'my_first',\n",
       " 'stopwords': 'russian',\n",
       " 'tf_model': {'max_features': 300}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  2,  2,  3,  9, 10,  2,  2,  2,  2], dtype=int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_matrix,\n",
    "                                                    cluster_labels,\n",
    "                                                    **config['cross_val'],\n",
    "                                                    random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lr = LogisticRegression(**config['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export MLFLOW_REGISTRY_URI=../mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/14 13:27:33 INFO mlflow.tracking.fluent: Experiment with name 'my_first' does not exist. Creating a new experiment.\n",
      "2024/10/14 13:27:33 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1019415  0.03471452 0.07495948 ... 0.03793634 0.20496989 0.05223601]\n",
      " [0.01806405 0.00804515 0.02716471 ... 0.00960577 0.28788865 0.01547912]\n",
      " [0.06885996 0.02905023 0.07846428 ... 0.0304531  0.04241483 0.04172083]\n",
      " ...\n",
      " [0.04285026 0.1252693  0.07856961 ... 0.04306601 0.04882859 0.04683564]\n",
      " [0.03794967 0.01268974 0.12699254 ... 0.02364957 0.02531088 0.06697331]\n",
      " [0.00598721 0.00329804 0.01601326 ... 0.00441986 0.02246577 0.00737974]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/14 13:27:36 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Successfully registered model 'vector_tfidf'.\n",
      "2024/10/14 13:27:36 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: vector_tfidf, version 1\n",
      "Created version '1' of model 'vector_tfidf'.\n",
      "2024/10/14 13:27:37 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Successfully registered model 'LogisticRegression'.\n",
      "2024/10/14 13:27:37 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: LogisticRegression, version 1\n",
      "Created version '1' of model 'LogisticRegression'.\n",
      "2024/10/14 13:27:37 INFO mlflow.tracking._tracking_service.client: 🏃 View run indecisive-sloth-676 at: http://localhost:5001/#/experiments/1/runs/eade7c2e56c440469bb41cf18f4b75cc.\n",
      "2024/10/14 13:27:37 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5001/#/experiments/1.\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:5001\")\n",
    "mlflow.set_experiment(config['name_experiment'])\n",
    "with mlflow.start_run():\n",
    "    clf_lr.fit(X_train, y_train)\n",
    "    print(clf_lr.predict_proba(X_test))\n",
    "\n",
    "    # Логирование модели и параметров\n",
    "    mlflow.log_param(\n",
    "        'f1', get_f1_score(y_test, clf_lr.predict(X_test),\n",
    "                           set(cluster_labels)))\n",
    "    mlflow.sklearn.log_model(\n",
    "        tfidf,\n",
    "        artifact_path=\"vector\",\n",
    "        registered_model_name=f\"{config['model_vec']}\")\n",
    "    mlflow.sklearn.log_model(\n",
    "        clf_lr,\n",
    "        artifact_path='model_lr',\n",
    "        registered_model_name=f\"{config['model_lr']}\")\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/forcemajor01/data_science/work_place/other/airflow-mlflow-tutorial/mlflow/1/96911c12d2284b2bbffff034b26b8e93/artifacts'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.get_artifact_uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_version_model(config_name, client):\n",
    "    \"\"\"\n",
    "    Получение последней версии модели из MLFlow\n",
    "    \"\"\"\n",
    "    dict_push = {}\n",
    "    for count, value in enumerate(\n",
    "        client.search_model_versions(f\"name='{config_name}'\")):\n",
    "        # client.list_registered_models()):\n",
    "        # Все версии модели\n",
    "        dict_push[count] = value\n",
    "    return dict(list(dict_push.items())[-1][1])['version']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient()\n",
    "last_version_lr = get_version_model(config['model_lr'], client)\n",
    "last_version_vec = get_version_model(config['model_vec'], client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_version_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_version_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
