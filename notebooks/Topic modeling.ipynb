{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import yaml\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "from googleapiclient.discovery import build\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import re\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import f1_score, silhouette_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ YouTube API\n",
    "def initialize_youtube(YOUTUBE_API_KEY):\n",
    "    return build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è ID –≤–∏–¥–µ–æ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º\n",
    "def get_video_ids(youtube, query, count_video=10):\n",
    "    \"\"\"\n",
    "    –ü–æ–∏—Å–∫ –≤–∏–¥–µ–æ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º\n",
    "    \"\"\"\n",
    "    search_response = youtube.search().list(\n",
    "        q=query,\n",
    "        part='id',\n",
    "        maxResults=count_video,\n",
    "        type='video'\n",
    "    ).execute()\n",
    "    \n",
    "    video_ids = [item['id']['videoId'] for item in search_response.get('items', [])]\n",
    "    return video_ids\n",
    "\n",
    "# –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö\n",
    "def get_data(YOUTUBE_API_KEY, videoId, maxResults, nextPageToken):\n",
    "    \"\"\"\n",
    "    –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –≤–∏–¥–µ–æ\n",
    "    \"\"\"\n",
    "    YOUTUBE_URI = 'https://www.googleapis.com/youtube/v3/commentThreads?key={KEY}&textFormat=plainText&' + \\\n",
    "        'part=snippet&videoId={videoId}&maxResults={maxResults}&pageToken={nextPageToken}'\n",
    "    format_youtube_uri = YOUTUBE_URI.format(KEY=YOUTUBE_API_KEY,\n",
    "                                            videoId=videoId,\n",
    "                                            maxResults=maxResults,\n",
    "                                            nextPageToken=nextPageToken)\n",
    "    content = requests.get(format_youtube_uri).text\n",
    "    data = json.loads(content)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_text_of_comment(data):\n",
    "    \"\"\"\n",
    "    –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∏–∑ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥ –æ–¥–Ω–∏–º –≤–∏–¥–µ–æ\n",
    "    \"\"\"\n",
    "    comms = set()\n",
    "    for item in data['items']:\n",
    "        comm = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "        comms.add(comm)\n",
    "    return comms\n",
    "\n",
    "\n",
    "# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤\n",
    "def get_all_comments(YOUTUBE_API_KEY, query, count_video=10, limit=30, maxResults=10, nextPageToken=''):\n",
    "    \"\"\"\n",
    "    –í—ã–≥—Ä—É–∑–∫–∞ maxResults –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤\n",
    "    \"\"\"\n",
    "    youtube = initialize_youtube(YOUTUBE_API_KEY)\n",
    "    videoIds = get_video_ids(youtube, query, count_video)\n",
    "\n",
    "    comments_all = []\n",
    "    for id_video in videoIds:\n",
    "        try:\n",
    "            data = get_data(YOUTUBE_API_KEY, id_video, maxResults=maxResults, nextPageToken=nextPageToken)\n",
    "            comment = list(get_text_of_comment(data))\n",
    "            comments_all.append(comment)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching comments for video ID {id_video}: {e}\")\n",
    "            continue\n",
    "    comments = sum(comments_all, [])\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join('/Users/forcemajor01/data_science/work_place/other/airflow-mlflow-tutorial/configs/params_all.yaml')\n",
    "config = yaml.safe_load(open(config_path))['train']\n",
    "SEED = config['SEED']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SEED': 10,\n",
       " 'clustering': {'affinity': 'cosine',\n",
       "  'count_max_clusters': 15,\n",
       "  'silhouette_metric': 'euclidean'},\n",
       " 'comments': {'YOUTUBE_API_KEY': 'AIzaSyBLU5mFczWyGRHq4HLpm9OzENB05l7RP3w',\n",
       "  'count_video': 50,\n",
       "  'limit': 30,\n",
       "  'maxResults': 250,\n",
       "  'nextPageToken': '',\n",
       "  'query': '–¥–∞—Ç–∞ —Å–∞–π–µ–Ω—Å'},\n",
       " 'cross_val': {'test_size': 0.3},\n",
       " 'dir_folder': '/Users/forcemajor01/data_science/work_place/other/airflow-mlflow-tutorial',\n",
       " 'model': {'class_weight': 'balanced'},\n",
       " 'model_lr': 'LogisticRegression',\n",
       " 'model_vec': 'vector_tfidf',\n",
       " 'name_experiment': 'my_first',\n",
       " 'stopwords': 'russian',\n",
       " 'tf_model': {'max_features': 300}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = get_all_comments(**config['comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['–¢–∞–∫ –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ',\n",
       " '–ù–∏–∫–∞–∫–æ–≥–æ –¥–µ—Ñ–∏—Ü–∏—Ç–∞ –≤ DS –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ –Ω–µ—Ç.\\n\\n–ß—Ç–æ –∫–∞—Å–∞–µ—Ç—Å—è entry-level –ø–æ–∑–∏—Ü–∏–π, —Ç–æ –æ–Ω–∏ –∑–∞–∫—Ä—ã–≤–∞—é—Ç—Å—è –ª–∏–±–æ –ø–æ –∑–Ω–∞–∫–æ–º—Å—Ç–≤–∞–º, –ª–∏–±–æ —Å –¥–∏—á–∞–π—à–∏–º –∫–æ–Ω–∫—É—Ä—Å–æ–º –≤ –ø–æ–ª—å–∑—É —Ä–µ–±—è—Ç —Å —Ç–æ–ø–æ–≤—ã—Ö –∫–∞—Ñ–µ–¥—Ä –ø–æ –º–∞—Ç–µ—à–µ/–ø—Ä–æ–≥–µ.\\n\\n –ü—Ä–æ–±–ª–µ–º–∞ —É —Ä–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª–µ–π —Ä–∞–∑–≤–µ —á—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å, –∫–∞–∫ –∏ –≤ –°–®–ê, –≤ –ø–æ–∏—Å–∫–µ —Å–ø–µ—Ü–æ–≤ –Ω–∞ research –ø–æ–∑–∏—Ü–∏–∏, —Ö–æ—Ç—è –¥–µ–Ω—å–≥–∏ —Ç–∞–º –±–æ–ª—å—à–∏–µ',\n",
       " '–†–∞–±–æ—Ç–∞—é –≤ —Å—Ñ–µ—Ä–µ –Ω–µ —Å–≤—è–∑–∞–Ω–Ω–æ–π —Å IT –∏ —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ –¥–∞–ª–µ–∫–æ–π –æ—Ç –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏, –∑–∞—Ä–ø–ª–∞—Ç–∞ –º–æ–∏ –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏ –ø–æ–∫—Ä—ã–≤–∞–µ—Ç, –Ω–æ –∑–∞—Ö–æ—Ç–µ–ª–æ—Å—å —á–µ–≥–æ-—Ç–æ –¥—Ä—É–≥–æ–≥–æ - –ø–æ –∏—Ç–æ–≥—É –ø–æ—à–µ–ª –Ω–∞ –∫—É—Ä—Å—ã –ø–æ DS –≤ –æ–¥–Ω—É –∏–∑–≤–µ—Å—Ç–Ω—É—é –∫–æ–Ω—Ç–æ—Ä—É. \\n–ó–∞–∫–∞–Ω—á–∏–≤–∞—é 2-–π –≥–æ–¥ –ø–æ CV. –ü—Ä–∏—à–µ–ª –∫ –≤—ã–≤–æ–¥—É, —á—Ç–æ –Ω—É–∂–Ω–∞ –≤—ã—à–∫–∞, –∏–Ω–∞—á–µ –±—É–¥—É—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å –ø–æ–∏—Å–∫–æ–º –Ω–æ–≤–æ–π —Ä–∞–±–æ—Ç—ã. \\n–°–¥–∞–ª —ç–∫–∑–∞–º–µ–Ω—ã –∏ –ø–æ—Å—Ç—É–ø–∏–ª –≤ –º–∞–≥–∏—Å—Ç—Ä–∞—Ç—É—Ä—É –æ–¥–Ω–æ–≥–æ –æ—á–µ–Ω—å –∏–∑–≤–µ—Å—Ç–Ω–æ–≥–æ –í–£–ó–∞ (–ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º —ç–∫–∑–∞–º–µ–Ω–æ–≤ –≤ –≤–µ—Ä—Ö–Ω–µ–π 1/3 –∞–±–∏—Ç—É—Ä–∏–µ–Ω—Ç–æ–≤). –° —Å–µ–Ω—Ç—è–±—Ä—è –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —É—á–µ–±–∞. \\n–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—á–µ–Ω—å –Ω—Ä–∞–≤–∏—Ç—Å—è, —Å –º–∞—Ç–µ–º–∞—Ç–∏–∫–æ–π, –ø—Ä–∞–≤–¥–∞, —Å–ª–æ–∂–Ω–æ, –Ω–æ –±—É–¥—É –ø—ã—Ç–∞—Ç—å—Å—è. \\n–°–æ —Å–≤–æ–µ–≥–æ –æ–ø—ã—Ç–∞ –æ–±—É—á–µ–Ω–∏—è —Å–∫–∞–∂—É —Ç–∞–∫ - –¥–∞—Ç–∞-—Å–∞–π–µ–Ω—Ç–∏—Å—Ç—ã —Å–≤–æ–∏ –∑–∞—Ä–ø–ª–∞—Ç—ã –ø–æ–ª—É—á–∞—é—Ç –Ω–µ –ø—Ä–æ—Å—Ç–æ —Ç–∞–∫, —ç—Ç–æ —Ä–µ–∞–ª—å–Ω–æ –æ—á–µ–Ω—å —Å–ª–æ–∂–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ. –ë–µ–∑ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏, –±–µ–∑ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ, –±–µ–∑ —á—Ç–µ–Ω–∏—è –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π —Ç—É—Ç –Ω–∏–∫—É–¥–∞. \\n–î–∞‚Ä¶ –µ—Å–ª–∏ –∫–æ–º—É –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ, –º–Ω–µ —Å–∫–æ—Ä–æ 45 :) –ù–µ –∑–Ω–∞—é, –∫–∞–∫ —É –º–µ–Ω—è –≤—Å–µ —Å–ª–æ–∂–∏—Ç—Å—è, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º —è –Ω–µ –∂–∞–ª–µ—é –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ä—É–±–ª—è, –ø–æ—Ç—Ä–∞—á–µ–Ω–Ω–æ–≥–æ –Ω–∞ —É—á–µ–±—É. \\n–í—Å–µ–º —É–¥–∞—á–∏!',\n",
       " '0:16 –≤–æ—Ç –≤—Å–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –∏ –∫–æ–Ω—á–∏–ª–∏—Å—å)))',\n",
       " '–ö–≤–∞—Ä—Ç–ê–ª! –ö–≤–∞—Ä—Ç–ê–ª!\\n–ê –Ω–µ –∫–≤–ê—Ä—Ç–∞–ª.\\n–û—Å—Ç–∞–ª—å–Ω–æ–µ - –±aza)',\n",
       " '–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –∏ –ø—Ä–∞–≤–¥–∞ —Ä–∞—Å—Ç–µ—Ç. –¢–∞–∫ —á—Ç–æ —á–µ—Ä–µ–∑ 4 –≥–æ–¥–∞ –º–æ–∂–Ω–æ –±—É–¥–µ—Ç —Å–ø–æ–∫–æ–π–Ω–æ –∞–≥–µ–Ω—Ç—Å—Ç–≤–æ –æ—Ç–∫—Ä—ã—Ç—å –∏–ª–∏ –Ω–∞ –∏–Ω—Ñ–æ–±–∏–∑ –ø–µ—Ä–µ–π—Ç–∏...',\n",
       " '–ê –µ—Å–ª–∏ —è –ª—é–±–ª—é –Ω–µ –º–∞—Ç–µ–º–∞—Ç–∏–∫—É, –¥–æ–∫–∞–ø—ã–≤–∞–Ω–∏–µ –¥–æ –∏—Ç—Å—Ç–∏–Ω–Ω—ã, –ø—Ä–æ–≥—Ä–∞–º–∏—Ä–æ–≤–∞–Ω–∏–µ, –∞ –ø—Ä–æ—Å—Ç–æ –ª—é–±–ª—é –¥–µ–Ω—å–≥–∏, –ø–æ–¥–æ–π–¥—ë—Ç –ø—Ä–æ—Ñ–µ—Å—Å–∏—è?',\n",
       " '–ù—É –ø–æ–Ω—è—Ç–Ω–æ —á—Ç–æ –≤—ã –≥–æ–≤–æ—Ä–∏—Ç–µ —á—Ç–æ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—ã data science –Ω—É–∂–Ω—ã, —á—Ç–æ –µ—Å—Ç—å –¥–µ—Ñ–∏—Ü–∏—Ç, —Ö–æ—Ç—è –µ–≥–æ –Ω–µ—Ç —Ç–∞–∫ –∫–∞–∫ —Å–ø–µ—Ü–∏–ª–∏—Å—Ç–æ–≤ –ø—Ä—è–º –¥–æ—Ö—Ä–µ–Ω–∞. –£ –≤–∞—Å —Å–∞–º–æ–π –≤–µ–¥—å –∫—É—Ä—Å –ø–æ ds',\n",
       " '–ê —è –≤–æ—Ç –∫–Ω–∏–∂–∫—É –ø—Ä–∏–∫—É–ø–∏–ª \"–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏\" –∞–≤—Ç–æ—Ä—ã: –î–∞–π–∑–µ–Ω—Ä–æ—Ç –ú–∞—Ä–∫ –ü–∏—Ç–µ—Ä, –ê–ª—å–¥–æ –§–µ–π–∑–∞–ª –ê., –ß–µ–Ω –°—É–Ω—å –û–Ω –¥–ª—è –Ω–æ–≤–∏—á–∫–∞ –ø–æ–¥–æ–π–¥–µ—Ç)',\n",
       " '–æ. –º–µ–º–Ω–æ –≤—Å—ë. –∫–∞–∫ –∫—Ä—É—Ç–æ. –Ω–∞–∫–æ–Ω–µ—Ü—Ç–æ –≤–µ—Å–µ–ª–æ —Å–º–æ—Ç—Ä–µ—Ç—å']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    \"\"\"\n",
    "    –£–¥–∞–ª–µ–Ω–∏–µ —ç–º–æ–¥–∂–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞\n",
    "    \"\"\"\n",
    "    emoji_pattern = re.compile(\"[\"u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u'\\U00010000-\\U0010ffff'\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\u3030\"\n",
    "                               u\"\\ufe0f\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "\n",
    "def remove_links(string):\n",
    "    \"\"\"\n",
    "    –£–¥–∞–ª–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫\n",
    "    \"\"\"\n",
    "    string = re.sub(r'http\\S+', '', string)  # remove http links\n",
    "    string = re.sub(r'bit.ly/\\S+', '', string)  # rempve bitly links\n",
    "    string = re.sub(r'www\\S+', '', string)  # rempve bitly links\n",
    "    string = string.strip('[link]')  # remove [links]\n",
    "    return string\n",
    "\n",
    "\n",
    "def preprocessing(string, stopwords, stem):\n",
    "    \"\"\"\n",
    "    –ü—Ä–æ—Å—Ç–æ–π –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞, –æ—á–∏—Å—Ç–∫–∞, –ª–µ–º–∞—Ç–∏–∑–∞—Ü–∏—è, —É–¥–∞–ª–µ–Ω–∏–µ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤\n",
    "    \"\"\"\n",
    "    string = remove_emoji(string)\n",
    "    string = remove_links(string)\n",
    "\n",
    "    # —É–¥–∞–ª–µ–Ω–∏–µ —Å–∏–º–≤–æ–ª–æ–≤ \"\\r\\n\"\n",
    "    str_pattern = re.compile(\"\\r\\n\")\n",
    "    string = str_pattern.sub(r'', string)\n",
    "\n",
    "    # –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç —Å–∏–º–≤–æ–ª–æ–≤\n",
    "    string = re.sub('(((?![–∞-—è–ê-–Ø ]).)+)', ' ', string)\n",
    "    # –ª–µ–º–∞—Ç–∏–∑–∞—Ü–∏—è\n",
    "    string = ' '.join([\n",
    "        re.sub('\\\\n', '', ' '.join(stem.lemmatize(s))).strip()\n",
    "        for s in string.split()\n",
    "    ])\n",
    "    # —É–¥–∞–ª—è–µ–º —Å–ª–æ–≤–∞ –∫–æ—Ä–æ—á–µ 3 —Å–∏–º–≤–æ–ª–æ–≤\n",
    "    string = ' '.join([s for s in string.split() if len(s) > 3])\n",
    "    # —É–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞\n",
    "    string = ' '.join([s for s in string.split() if s not in stopwords])\n",
    "    return string\n",
    "\n",
    "\n",
    "def get_clean_text(data, stopwords):\n",
    "    \"\"\"\n",
    "    –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω–æ–π –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏\n",
    "    –º–∞—Ç—Ä–∏—á–Ω–æ–º –≤–∏–¥–µ, –∞ —Ç–∞–∫–∂–µ –º–æ–¥–µ–ª—å –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏\n",
    "    \"\"\"\n",
    "    # –ü—Ä–æ—Å—Ç–æ–π –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞\n",
    "    stem = Mystem()\n",
    "    comments = [preprocessing(x, stopwords, stem) for x in data]\n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ –∫–æ–º–º–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç –º–µ–Ω—å—à–µ, —á–µ–º 5 —Å–ª–æ–≤\n",
    "    comments = [y for y in comments if len(y.split()) > 5]\n",
    "    #common_texts = [i.split(' ') for i in comments]\n",
    "    return comments\n",
    "\n",
    "\n",
    "def vectorize_text(data, tfidf):\n",
    "    \"\"\"\n",
    "    –ü–æ–ª—É—á–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –∫–æ–ª-–≤–∞ —Å–ª–æ–≤ –≤ –∫–æ–º–º–µ–Ω–∞—Ä–∏—è—Ö\n",
    "    –û—á–∏—Å—Ç–∫–∞ –æ—Ç –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫\n",
    "    \"\"\"\n",
    "    # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è\n",
    "    X_matrix = tfidf.transform(data).toarray()\n",
    "    # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –≤ –º–∞—Ç—Ä–∏—Ü–µ —Å –ø—É—Å—Ç—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏\n",
    "    mask = (np.nan_to_num(X_matrix) != 0).any(axis=1)\n",
    "    return X_matrix[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_clean = get_clean_text(comments, stopwords.words(config['stopwords']))\n",
    "tfidf = TfidfVectorizer(**config['tf_model']).fit(comments_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['–Ω–∏–∫–∞–∫–æ–π –¥–µ—Ñ–∏—Ü–∏—Ç —Å–∞–º—ã–π –¥–µ–ª–æ –∫–∞—Å–∞—Ç—å—Å—è –ø–æ–∑–∏—Ü–∏—è –∑–∞–∫—Ä—ã–≤–∞—Ç—å—Å—è –ª–∏–±–æ –∑–Ω–∞–∫–æ–º—Å—Ç–≤–æ –ª–∏–±–æ –¥–∏–∫–∏–π –∫–æ–Ω–∫—É—Ä—Å –ø–æ–ª—å–∑–∞ —Ä–µ–±—è—Ç–∞ —Ç–æ–ø–æ–≤—ã–π –∫–∞—Ñ–µ–¥—Ä–∞ –º–∞—Ç–µ—à –ø—Ä–æ–≥–∞ –ø—Ä–æ–±–ª–µ–º–∞ —Ä–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª—å –ø–æ–∏—Å–∫ —Å–ø–µ—Ü –ø–æ–∑–∏—Ü–∏—è —Ö–æ—Ç—è –¥–µ–Ω—å–≥–∏ –±–æ–ª—å—à–æ–π',\n",
       " '—Ä–∞–±–æ—Ç–∞—Ç—å —Å—Ñ–µ—Ä–∞ —Å–≤—è–∑—ã–≤–∞—Ç—å —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ –¥–∞–ª–µ–∫–∏–π –º–∞—Ç–µ–º–∞—Ç–∏–∫ –∑–∞—Ä–ø–ª–∞—Ç–∞ –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—å –ø–æ–∫—Ä—ã–≤–∞—Ç—å –∑–∞—Ö–æ—Ç–µ—Ç—å—Å—è –∏—Ç–æ–≥ –ø–æ–π—Ç–∏ –∫—É—Ä—Å—ã –∏–∑–≤–µ—Å—Ç–Ω—ã–π –∫–æ–Ω—Ç–æ—Ä–∞ –∑–∞–∫–∞–Ω—á–∏–≤–∞—Ç—å –ø—Ä–∏—Ö–æ–¥–∏—Ç—å –≤—ã–≤–æ–¥ –Ω—É–∂–Ω—ã–π –≤—ã—à–∫–∞ –∏–Ω–∞—á–µ –ø—Ä–æ–±–ª–µ–º–∞ –ø–æ–∏—Å–∫ –Ω–æ–≤—ã–π —Ä–∞–±–æ—Ç–∞ —Å–¥–∞–≤–∞—Ç—å —ç–∫–∑–∞–º–µ–Ω –ø–æ—Å—Ç—É–ø–∞—Ç—å –º–∞–≥–∏—Å—Ç—Ä–∞—Ç—É—Ä–∞ –æ—á–µ–Ω—å –∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —ç–∫–∑–∞–º–µ–Ω –≤–µ—Ä—Ö–Ω–∏–π –∞–±–∏—Ç—É—Ä–∏–µ–Ω—Ç —Å–µ–Ω—Ç—è–±—Ä—å –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —É—á–µ–±–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—á–µ–Ω—å –Ω—Ä–∞–≤–∏—Ç—å—Å—è –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –ø—Ä–∞–≤–¥–∞ —Å–ª–æ–∂–Ω–æ –ø—ã—Ç–∞—Ç—å—Å—è —Å–≤–æ–π –æ–ø—ã—Ç –æ–±—É—á–µ–Ω–∏–µ —Å–∫–∞–∑–∞—Ç—å –¥–∞—Ç–∞ —Å–∞–π–µ–Ω—Ç–∏—Å—Ç —Å–≤–æ–π –∑–∞—Ä–ø–ª–∞—Ç–∞ –ø–æ–ª—É—á–∞—Ç—å –ø—Ä–æ—Å—Ç–æ —Ä–µ–∞–ª—å–Ω–æ –æ—á–µ–Ω—å —Å–ª–æ–∂–Ω—ã–π –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–∞—Ç–µ–º–∞—Ç–∏–∫ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —á—Ç–µ–Ω–∏–µ –Ω–∞—É—á–Ω—ã–π —Å—Ç–∞—Ç—å—è –Ω–∏–∫—É–¥–∞ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ —Å–∫–æ—Ä–æ –∑–Ω–∞—Ç—å —Å–∫–ª–∞–¥—ã–≤–∞—Ç—å—Å—è –∂–∞–ª–µ—Ç—å —Ä—É–±–ª—å –ø–æ—Ç—Ä–∞—Ç–∏—Ç—å —É—á–µ–±–∞ —É–¥–∞—á–∞',\n",
       " '–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –ø—Ä–∞–≤–¥–∞ —Ä–∞—Å—Ç–∏ —Å–ø–æ–∫–æ–π–Ω–æ –∞–≥–µ–Ω—Ç—Å—Ç–≤–æ –æ—Ç–∫—Ä—ã–≤–∞—Ç—å –∏–Ω—Ñ–æ–±–∏–∑ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å',\n",
       " '–ª—é–±–∏—Ç—å –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –¥–æ–∫–∞–ø—ã–≤–∞–Ω–∏–µ –∏—Ç—Å—Ç–∏–Ω–Ω—ã–π –ø—Ä–æ–≥—Ä–∞–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ –ª—é–±–∏—Ç—å –¥–µ–Ω—å–≥–∏ –ø–æ–¥–æ–π–¥ –ø—Ä–æ—Ñ–µ—Å—Å–∏—è',\n",
       " '–ø–æ–Ω—è—Ç–Ω–æ –≥–æ–≤–æ—Ä–∏—Ç—å —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –Ω—É–∂–Ω—ã–π –¥–µ—Ñ–∏—Ü–∏—Ç —Ö–æ—Ç—è —Å–ø–µ—Ü–∏–ª–∏—Å—Ç –ø—Ä—è–º–æ–π –¥–æ—Ö—Ä–∏—Ç—å —Å–∞–º—ã–π –∫—É—Ä—Å',\n",
       " '–∫–Ω–∏–∂–∫–∞ –ø—Ä–∏–∫—É–ø–∞—Ç—å –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –º–∞—à–∏–Ω–Ω—ã–π –æ–±—É—á–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä –¥–∞–π–∑–µ–Ω—Ä–æ—Ç –º–∞—Ä–∫ –ø–∏—Ç–µ—Ä –∞–ª—å–¥–æ —Ñ–µ–π–∑–∞–ª —Å—É–Ω—É—Ç—å –Ω–æ–≤–∏—á–æ–∫ –ø–æ–¥—Ö–æ–¥–∏—Ç—å',\n",
       " '—Å–º–æ—Ç—Ä–µ—Ç—å –º–µ—Å—è—Ü –º–æ—á—å –Ω–∞—Ö–æ–¥–∏—Ç—å —Ä–∞–±–æ—Ç–∞ –≤–µ–∑–¥–µ –Ω—É–∂–Ω—ã–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –æ–ø—ã—Ç –¥–∂—É–Ω–∞ –≤–æ—Å—Ç—Ä–µ–±–æ–≤–∞—Ç—å',\n",
       " '—Å–º–µ—à–∏—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç –¥–µ—Å–∫–∞—Ç—å —Ö–æ—Ä–æ—à–∏–π —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –≤–∞–∂–Ω—ã–π —Ä–∞–±–æ—Ç–∞ —Ä–∞–≤–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç—å –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å —Ä—É—Å—Å–∫–∏–π –∑–Ω–∞—á–∏—Ç –Ω—É–∂–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å –º–∞–ª–æ –¥–µ–Ω—å–≥–∏ –≤–∞–∫–Ω—Å–∏—è –æ—Å—Ç–∞–≤–∞—Ç—å—Å—è',\n",
       " '–ª—é–±–∏—Ç—å –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –ø–æ–∫–∞ –¥–æ—Ö–æ–¥–∏—Ç—å —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü–∞ –ø–æ–∫–∞ –æ—Å—Ç–∞–≤–ª—è—Ç—å —É—á–∏—Ç—å—Å—è —Ñ–∏–Ω–∞–Ω—Å—ã –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –Ω—É–∂–Ω—ã–π —Ö–æ—Ç–µ—Ç—å –±—É–¥—É—â–µ–µ —Å–≤—è–∑—ã–≤–∞—Ç—å —Å—Ñ–µ—Ä–∞ —Ñ–∏–Ω–∞–Ω—Å—ã –Ω–∞—É–∫–∞ –¥–∞–Ω–Ω—ã–π –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∑–∞–ø–∏—Å—ã–≤–∞—Ç—å—Å—è –∫—É—Ä—Å —Ö–æ—Ä–æ—à–∏–π —Ä–µ–±—è—Ç–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ —Å–ø–∞—Å–∏–±–æ –≤–∏–¥–µ–æ –Ω–µ–º–Ω–æ–≥–æ –º–æ—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å',\n",
       " '—Ä–∞–¥–∏ –ª—é–±–æ–ø—ã—Ç—Å—Ç–≤–æ –∑–∞–π—Ç–∏ –≤–∞–∫–∞–Ω—Å–∏—è –≤–∞–∫–∞–Ω—Å–∏—è –Ω–µ–¥–µ–ª—è —Ä–µ–∑—é–º–µ –∏—Å–∫–∞—Ç—å –ø–æ—Ö–æ–∂–µ –æ—Å–æ–±—ã–π –¥–µ—Ñ–∏—Ü–∏—Ç']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_clean[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_matrix = vectorize_text(comments_clean, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1132, 300)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['–∞–≤—Ç–æ—Ä', '–∞–ª–≥–æ—Ä–∏—Ç–º', '–∞–Ω–∞–ª–∏–∑', '–∞–Ω–∞–ª–∏—Ç–∏–∫', '–∞–Ω–∞–ª–∏—Ç–∏–∫–∞',\n",
       "       '–∞–Ω–∞—Å—Ç–∞—Å–∏—è', '–∞–Ω–≥–ª–∏–π—Å–∫–∏–π', '–±–∞–±—É—à–∫–∏–Ω', '–±–∞–∑–∞', '–±–∞–∑–æ–≤—ã–π'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names_out()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(data, count_max_clusters, random_state, affinity,\n",
    "                 silhouette_metric):\n",
    "    \"\"\"\n",
    "    –ü–æ–¥–±–æ—Ä –Ω–∞–∏–ª—É—á—à–µ–≥–æ —á–∏—Å–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä–∞ —Ç–µ–º–∞—Ç–∏–∫\n",
    "    \"\"\"\n",
    "    cluster_labels = {}\n",
    "    silhouette_mean = []\n",
    "\n",
    "    for i in range(2, count_max_clusters, 1):\n",
    "        clf = SpectralClustering(n_clusters=i,\n",
    "                                 affinity=affinity,\n",
    "                                 random_state=random_state)\n",
    "        #clf = KMeans(n_clusters=n, max_iter=1000, n_init=1)\n",
    "        clf.fit(data)\n",
    "        labels = clf.labels_\n",
    "        cluster_labels[i] = labels\n",
    "        silhouette_mean.append(\n",
    "            silhouette_score(data, labels, metric=silhouette_metric))\n",
    "    n_clusters = silhouette_mean.index(max(silhouette_mean)) + 2\n",
    "    return cluster_labels[n_clusters]\n",
    "\n",
    "\n",
    "def get_f1_score(y_test, y_pred, unique_cluster_labels):\n",
    "    \"\"\"\n",
    "    –í–æ–∑—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—É—á–µ–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –ø–æ —Ç–µ–º–∞—Ç–∏–∫–∞–º\n",
    "    \"\"\"\n",
    "    return f1_score(\n",
    "        y_test, y_pred,\n",
    "        average='macro') \\\n",
    "        if len(unique_cluster_labels) > 2 \\\n",
    "        else f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = get_clusters(X_matrix,\n",
    "                                 random_state=SEED,\n",
    "                                 **config['clustering'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SEED': 10,\n",
       " 'clustering': {'affinity': 'cosine',\n",
       "  'count_max_clusters': 15,\n",
       "  'silhouette_metric': 'euclidean'},\n",
       " 'comments': {'YOUTUBE_API_KEY': 'AIzaSyBLU5mFczWyGRHq4HLpm9OzENB05l7RP3w',\n",
       "  'count_video': 50,\n",
       "  'limit': 30,\n",
       "  'maxResults': 250,\n",
       "  'nextPageToken': '',\n",
       "  'query': '–¥–∞—Ç–∞ —Å–∞–π–µ–Ω—Å'},\n",
       " 'cross_val': {'test_size': 0.3},\n",
       " 'dir_folder': '/Users/forcemajor01/data_science/work_place/other/airflow-mlflow-tutorial',\n",
       " 'model': {'class_weight': 'balanced'},\n",
       " 'model_lr': 'LogisticRegression',\n",
       " 'model_vec': 'vector_tfidf',\n",
       " 'name_experiment': 'my_first',\n",
       " 'stopwords': 'russian',\n",
       " 'tf_model': {'max_features': 300}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  2,  2,  3,  9, 10,  2,  2,  2,  2], dtype=int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_matrix,\n",
    "                                                    cluster_labels,\n",
    "                                                    **config['cross_val'],\n",
    "                                                    random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lr = LogisticRegression(**config['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export MLFLOW_REGISTRY_URI=../mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/14 13:27:33 INFO mlflow.tracking.fluent: Experiment with name 'my_first' does not exist. Creating a new experiment.\n",
      "2024/10/14 13:27:33 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1019415  0.03471452 0.07495948 ... 0.03793634 0.20496989 0.05223601]\n",
      " [0.01806405 0.00804515 0.02716471 ... 0.00960577 0.28788865 0.01547912]\n",
      " [0.06885996 0.02905023 0.07846428 ... 0.0304531  0.04241483 0.04172083]\n",
      " ...\n",
      " [0.04285026 0.1252693  0.07856961 ... 0.04306601 0.04882859 0.04683564]\n",
      " [0.03794967 0.01268974 0.12699254 ... 0.02364957 0.02531088 0.06697331]\n",
      " [0.00598721 0.00329804 0.01601326 ... 0.00441986 0.02246577 0.00737974]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/14 13:27:36 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Successfully registered model 'vector_tfidf'.\n",
      "2024/10/14 13:27:36 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: vector_tfidf, version 1\n",
      "Created version '1' of model 'vector_tfidf'.\n",
      "2024/10/14 13:27:37 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Successfully registered model 'LogisticRegression'.\n",
      "2024/10/14 13:27:37 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: LogisticRegression, version 1\n",
      "Created version '1' of model 'LogisticRegression'.\n",
      "2024/10/14 13:27:37 INFO mlflow.tracking._tracking_service.client: üèÉ View run indecisive-sloth-676 at: http://localhost:5001/#/experiments/1/runs/eade7c2e56c440469bb41cf18f4b75cc.\n",
      "2024/10/14 13:27:37 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://localhost:5001/#/experiments/1.\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:5001\")\n",
    "mlflow.set_experiment(config['name_experiment'])\n",
    "with mlflow.start_run():\n",
    "    clf_lr.fit(X_train, y_train)\n",
    "    print(clf_lr.predict_proba(X_test))\n",
    "\n",
    "    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "    mlflow.log_param(\n",
    "        'f1', get_f1_score(y_test, clf_lr.predict(X_test),\n",
    "                           set(cluster_labels)))\n",
    "    mlflow.sklearn.log_model(\n",
    "        tfidf,\n",
    "        artifact_path=\"vector\",\n",
    "        registered_model_name=f\"{config['model_vec']}\")\n",
    "    mlflow.sklearn.log_model(\n",
    "        clf_lr,\n",
    "        artifact_path='model_lr',\n",
    "        registered_model_name=f\"{config['model_lr']}\")\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/forcemajor01/data_science/work_place/other/airflow-mlflow-tutorial/mlflow/1/96911c12d2284b2bbffff034b26b8e93/artifacts'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.get_artifact_uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_version_model(config_name, client):\n",
    "    \"\"\"\n",
    "    –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤–µ—Ä—Å–∏–∏ –º–æ–¥–µ–ª–∏ –∏–∑ MLFlow\n",
    "    \"\"\"\n",
    "    dict_push = {}\n",
    "    for count, value in enumerate(\n",
    "        client.search_model_versions(f\"name='{config_name}'\")):\n",
    "        # client.list_registered_models()):\n",
    "        # –í—Å–µ –≤–µ—Ä—Å–∏–∏ –º–æ–¥–µ–ª–∏\n",
    "        dict_push[count] = value\n",
    "    return dict(list(dict_push.items())[-1][1])['version']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient()\n",
    "last_version_lr = get_version_model(config['model_lr'], client)\n",
    "last_version_vec = get_version_model(config['model_vec'], client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_version_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_version_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
